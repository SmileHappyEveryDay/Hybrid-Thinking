import os
import sys
import json
import subprocess
import argparse
import re  # å¿…é¡»å¯¼å…¥
from tqdm import tqdm

# === é…ç½®æ‚¨ä¸‹è½½çš„æœ¬åœ°è·¯å¾„ ===
LCB_PKG_DIR = "/LiveCodeBench_pkg"


# ==========================

def extract_code(text: str) -> str:
    """æ›´é²æ£’çš„ä»£ç æå–å‡½æ•°"""
    if not isinstance(text, str):
        return ""

    # 1. ç§»é™¤ <think> æ ‡ç­¾
    if "<think>" in text:
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

    # 2. å°è¯•æå–ä»£ç å— (æ”¯æŒ ```python, ``` python, ```Python ç­‰)
    # pattern: ``` + ä»»æ„ç©ºç™½ + (å¯é€‰è¯­è¨€å) + æ¢è¡Œ + (ä»£ç å†…å®¹) + ```
    matches = re.findall(r"```(?:python|py)?\s*\n(.*?)```", text, re.IGNORECASE | re.DOTALL)
    if matches:
        return matches[-1].strip()

    # 3. é€šç”¨ fallback (åŒ¹é…ä»»ä½• ``` ... ```)
    matches_generic = re.findall(r"```\s*\n(.*?)```", text, re.DOTALL)
    if matches_generic:
        return matches_generic[-1].strip()

    # 4. å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•ç®€å•çš„å¯å‘å¼æ¸…æ´—ï¼ˆå¦‚æœåªæœ‰ä»£ç ï¼‰
    # æˆ–è€…ç›´æ¥è¿”å›åŸå§‹å†…å®¹
    return text.strip()


def convert_to_lcb_format(input_file, output_file):
    """å°†ç»“æœè½¬æ¢ä¸º LiveCodeBench éœ€è¦çš„æ ¼å¼ (å«ä»£ç æ¸…æ´—)"""
    print(f"Converting {input_file} -> {output_file}")
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        output_data = []
        for item in data:
            # è·å– question_id
            gt = item.get("ground_truth")
            qid = None
            if isinstance(gt, dict):
                qid = gt.get("question_id")

            if not qid:
                continue

            # è·å–åŸå§‹è¾“å‡º
            completion = item.get("completion")

            # æå–ä»£ç 
            if isinstance(completion, str):
                completion_list = [extract_code(completion)]
            elif isinstance(completion, list):
                completion_list = [extract_code(c) for c in completion]
            else:
                completion_list = [""]

            # LCB æ ¼å¼è¦æ±‚
            output_data.append({
                "question_id": qid,
                "code_list": completion_list
            })

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"Format conversion failed: {e}")
        return False


def update_stats(result_file, acc, updated_data):
    """åŒæ­¥æ›´æ–° statistics æ–‡ä»¶"""
    stats_file = result_file.replace(".json", "_statistics.json")
    stats = {}

    # è¯»å–æ—§æ•°æ®
    if os.path.exists(stats_file):
        try:
            with open(stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)
        except:
            pass

    # æ›´æ–°å…³é”®æŒ‡æ ‡
    stats["pass@1"] = acc
    stats["total_num"] = len(updated_data)

    # é‡æ–°è®¡ç®— Token é•¿åº¦
    all_tokens = []
    correct_tokens = []
    for item in updated_data:
        t_len = item.get("avg_generated_tokens", 0)
        if t_len == 0 and isinstance(item.get("generated_tokens"), list) and item["generated_tokens"]:
            t_len = sum(item["generated_tokens"]) / len(item["generated_tokens"])

        all_tokens.append(t_len)
        if item.get("passat1", 0) > 0:
            correct_tokens.append(t_len)

    stats["avg_token_length-all"] = sum(all_tokens) / len(all_tokens) if all_tokens else 0
    stats["avg_token_length-correct"] = sum(correct_tokens) / len(correct_tokens) if correct_tokens else 0

    # æ›´æ–°ç´¢å¼•
    updated_data.sort(key=lambda x: x.get("idx", 0))
    stats["all_idx"] = {str(d.get("idx")): d.get("passat1", 0.0) for d in updated_data}

    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=4)
    print(f"Stats file updated: {stats_file}")


def evaluate_single_file(filepath):
    print(f"\nProcessing: {filepath}")

    # 1. å‡†å¤‡è·¯å¾„
    abs_filepath = os.path.abspath(filepath)
    converted_file = abs_filepath.replace(".json", "_converted.json")

    # 2. è½¬æ¢æ ¼å¼
    if not convert_to_lcb_format(abs_filepath, converted_file):
        return 0.0  # å¤±è´¥è¿”å› 0

    # 3. å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
    if not os.path.exists(LCB_PKG_DIR):
        print(f"âŒ Error: æœ¬åœ°åº“è·¯å¾„ä¸å­˜åœ¨: {LCB_PKG_DIR}")
        return 0.0

    orig_cwd = os.getcwd()
    try:
        os.chdir(LCB_PKG_DIR)

        # æ„é€ å‘½ä»¤
        cmd = [
            sys.executable, "-m", "lcb_runner.runner.custom_evaluator",
            "--custom_output_file", converted_file,
            "--release_version", "release_v5",
            "--start_date", "2024-08-01",
            "--num_process_evaluate", "8",
            "--timeout", "60"
        ]

        # è®¾ç½®ç¼“å­˜ç¯å¢ƒ
        env = os.environ.copy()
        temp_cache_dir = "/root/shared-nvme/gj/tmp/lcb_cache_safe"
        os.makedirs(temp_cache_dir, exist_ok=True)
        env["HF_DATASETS_CACHE"] = temp_cache_dir
        env["HF_HOME"] = temp_cache_dir

        print("ğŸš€ Running LCB Runner (Local)...")
        subprocess.run(cmd, check=True, env=env)

    except subprocess.CalledProcessError as e:
        print(f"âŒ Evaluation failed: {e}")
        return 0.0
    finally:
        os.chdir(orig_cwd)

    # 4. åˆå¹¶ç»“æœ
    output_eval_file = converted_file.replace(".json", "_codegeneration_output_eval_all.json")

    if not os.path.exists(output_eval_file):
        print(f"âŒ Output file not found: {output_eval_file}")
        return 0.0

    print("ğŸ”„ Merging results...")
    try:
        with open(output_eval_file, 'r', encoding='utf-8') as f:
            lcb_results = json.load(f)
        with open(abs_filepath, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
    except Exception as e:
        print(f"âŒ Error reading result files: {e}")
        return 0.0

    pass_list = []
    updated_data = []

    # åˆ›å»ºå¿«é€ŸæŸ¥æ‰¾å­—å…¸
    lcb_map = {res["question_id"]: res for res in lcb_results}

    for orig in original_data:
        qid = None
        if isinstance(orig.get("ground_truth"), dict):
            qid = orig["ground_truth"].get("question_id")

        if qid and qid in lcb_map:
            res = lcb_map[qid]
            # ç¡®ä¿è½¬æ¢ä¸º float
            score = float(res.get("pass@1", 0))
            orig["passat1"] = score
            orig["judge_info"] = res.get("metadata", {})
            pass_list.append(score)
            updated_data.append(orig)
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°ç»“æœï¼Œè®°ä¸º 0
            orig["passat1"] = 0.0
            updated_data.append(orig)

    # 5. å†™å›åŸå§‹æ–‡ä»¶å¹¶è¿”å›åˆ†æ•°
    acc = 0.0
    if pass_list:
        acc = sum(pass_list) / len(pass_list)
        print(f"âœ… Evaluation Complete. Pass@1: {acc:.2%}")

        with open(abs_filepath, 'w', encoding='utf-8') as f:
            json.dump(original_data, f, indent=4, ensure_ascii=False)

        update_stats(abs_filepath, acc, updated_data)
    else:
        print("âš ï¸ Warning: No matching results found during merge.")

    return acc  # <--- ã€å…³é”®ä¿®æ”¹ã€‘å¿…é¡»è¿”å› accï¼Œå¦åˆ™å¤–é¢æ”¶åˆ°çš„æ˜¯ None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--scan_dir", type=str, required=True, help="Directory containing result jsons")
    args = parser.parse_args()

    if not os.path.exists(args.scan_dir):
        print(f"Directory not found: {args.scan_dir}")
        return

    print(f"Scanning {args.scan_dir} for livecodebench results...")

    found = False
    for root, dirs, files in os.walk(args.scan_dir):
        for file in files:
            if not file.endswith(".json"): continue
            if "statistics" in file or "converted" in file or "codegeneration" in file: continue

            if "livecodebench" in file.lower() or "livecodebench" in root.lower():
                filepath = os.path.join(root, file)
                evaluate_single_file(filepath)
                found = True

    if not found:
        print("No livecodebench result files found.")


if __name__ == "__main__":
    main()